\documentclass{beamer}

\usepackage[english]{babel}
\usepackage{xcolor}
\usepackage{xmpmulti}
\usepackage{amsmath}
\usepackage{dsfont}
\usepackage{multicol}
\usepackage{tikz}
\usepackage{eucal}
\usetikzlibrary{positioning,angles,quotes}
\usepackage{url}
\usepackage{graphicx}
\usepackage{cmbright}
\usepackage{framed}


\usetikzlibrary{pgfplots.groupplots,arrows.meta,shadows,positioning,angles,quotes}
\usetikzlibrary{matrix,chains,positioning,decorations.pathreplacing,arrows}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{positioning}
\usepackage{pgfplots}

%\input{epgfplotslibrary{groupplots}
\usetikzlibrary{pgfplots.groupplots,arrows.meta,shadows,positioning,angles,quotes}
\usetikzlibrary{matrix,chains,positioning,decorations.pathreplacing,arrows}



\DeclareMathOperator*{\argmax}{arg\,max}
\def\checkmark{\tikz\fill[scale=0.4](0,.35) -- (.25,0) -- (1,.7) -- (.25,.15) -- cycle;} 

\definecolor{Maroon}{cmyk}{0, 0.87, 0.68, 0.32}
\definecolor{RoyalBlue}{cmyk}{1, 0.50, 0, 0}
\definecolor{skymagenta}{rgb}{0.81, 0.44, 0.69}

\newenvironment{takeaway}[1]{%
	\definecolor{shadecolor}{gray}{0.9}%
		\begin{shaded}{\color{skymagenta}\noindent\textsc{#1}}\\%
		}{%
		\end{shaded}%
}



%%%%%% THE FOLLOWING FILE CONTAINS THE STYLE DEFINITIONS %%%%%%
\input{header.tex}
%%%%%%

%%%%%% TITLE, AUTHOR, DATE DEFINITIONS %%%%%%
\title{Foundations of Reinforcement Learning}
\subtitle{From Markov Decision Processes to Optimal Value Functions}
\author{Matthia Sabatelli}


\date{\today}
%%%%%%

\setbeamertemplate{footline}[frame number]{}

\begin{document}

\frame{\titlepage} 

\frame{\frametitle{Today's Agenda}\tableofcontents}

%============================================================================
\begin{frame}{Course Information}
	\section{Course Information}

	\begin{itemize}
		\item Coordinator: Matthia Sabatelli
		\item Lecturers: Matthia Sabatelli (\textcolor{RoyalBlue}{m.sabatelli@rug.nl}) and Nicole Orzan (\textcolor{RoyalBlue}{n.orzan@rug.nl})
		\item Classroom: TBD
		\item Theoretical Lectures: Monday morning from 9:00-11:00
		\item Computer Labs: Monday afternoon from 15:00-17:00
	\end{itemize}

\end{frame}


\begin{frame}{Course Information}

	\begin{itemize}
		\item Lecture 1: Foundations of Reinforcement Learning (Matthia)
		\item Lecture 2: Exploration and Bandit Problems (Nicole)
		\item Lecture 3: Dynamic Programming (Nicole)
		\item Lecture 4: Model-Free Reinforcement Learning (Matthia)
		\item Lecture 5: Function Approximators (Matthia) 
		\item Lecture 6: Beyond Model-Free Reinforcement Learning (Matthia)
		\item Lecture 7: \textit{What does it mean to do research in RL?} (Matthia \& Nicole)
	\end{itemize}

\end{frame}

\begin{frame}{Course Information}

	All \textcolor{RoyalBlue}{course material} will be made available

	\begin{itemize}
		\item Nestor
		\item Github: \url{https://github.com/paintception/reinforcement-learning-practical}
	\end{itemize}

\end{frame}

\begin{frame}{Course Information}
	\textcolor{RoyalBlue}{Textbook}: Reinforcement Learning: An Introduction by Sutton \& Barto

	\centering
	\includegraphics[scale=0.25]{./images/sutton_pic}

\end{frame}

\begin{frame}{Course Information}
	Final course \textcolor{RoyalBlue}{assessement}:
	\begin{itemize}
		\item There is \textbf{no exam}
		\item Students should handle in three deliverables:
			\begin{enumerate}
				\item Assignment 1: $25\%$ of the grade (coding)
				\item Assignment 2: $25\%$ of the grade (mathematics)
				\item Report: $50\%$ of the grade (final project)
			\end{enumerate}
		\item Students can work alone or in groups of a maximum of \textbf{2} people 
	\end{itemize}

\end{frame}

%============================================================================






%============================================================================
\begin{frame}{Reinforcement Learning}
	\section{What is Reinforcement Learning}
	
	Machine Learning is typically divided into three branches:

	\begin{enumerate}
		\item Supervised Learning: learning from \textcolor{RoyalBlue}{labeled} data
		\item Unsupervised Learning: learning from \textcolor{RoyalBlue}{unlabeled} data
		\item Reinforcement Learning: learning from \textcolor{RoyalBlue}{experience}
	\end{enumerate}

\end{frame}


\begin{frame}{Reinforcement Learning}
	A reminder of \textcolor{RoyalBlue}{supervised learning}:
	\begin{itemize}
		\item Input space: $\mathcal{X}$
		\item Output space: $\mathcal{Y}$
		\item Probability distribution $p(x,y)$
	\end{itemize}

	\begin{block}{The goal}
		We want to build a function $f:\mathcal{X}\rightarrow\mathcal{Y}$ that minimizes the \textcolor{Maroon}{expectation} of a given loss $\ell$
		\begin{align*}
			\mathds{E}_{(x,y)\sim p(x,y)}\{\ell(y,f(x)) \}
		\end{align*}
		through leaning samples $LS=\{(x_i,y_i)|i=1,...,N\}$ of input-ouput pairs drawn from $p(x,y)$.
	\end{block}

\end{frame}

\begin{frame}{Reinforcement Learning}
	Supervised learning is therefore:
	\begin{itemize}
		\item A learning paradigm which is \textcolor{Maroon}{static}
		\item \textcolor{Maroon}{No interaction} happens between the learner and $p(x,y)$
		\item Assumes we have access to a \textcolor{Maroon}{knowledgable supervisor}
	\end{itemize}

	\bigskip
	In Reinforcement Learning however ...

\end{frame}

\begin{frame}{Reinforcement Learning}
	\begin{itemize}
		\item We would like to learn how to \textcolor{RoyalBlue}{interact} with an environment 
		\item We do not assume any sort of supervision but only a \textcolor{RoyalBlue}{reward} signal
		\item The component of \textcolor{RoyalBlue}{time} plays a crucial role
		\item The learning process is therefore \textcolor{RoyalBlue}{dynamic} and \textcolor{Maroon}{uncertain} 
		\item Agents are \textcolor{RoyalBlue}{goal oriented}
	\end{itemize}

\end{frame}

\begin{frame}{Reinforcement Learning}
	\centering
	\input{./images/rl_loop.tex}
\end{frame}

\begin{frame}{Reinforcement Learning}
Some examples of Reinforcement Learning:
	\begin{itemize}
		\item \url{https://www.youtube.com/watch?v=eG1Ed8PTJ18}
		\item \url{https://www.youtube.com/watch?v=n2gE7n11h1Y}
		\item \url{https://www.youtube.com/watch?v=_qLs2K4UXXk}
	\end{itemize}

\end{frame}


\begin{frame}{Reinforcement Learning}
	The mathematical framework of Reinforcement Learning:
	\begin{itemize}
		\item A set of possible \textcolor{RoyalBlue}{states} $\mathcal{S}$ where $s_t\in\mathcal{S}$ is the current state
		\item A set of possible \textcolor{RoyalBlue}{actions} $\mathcal{A}$ where $a_t\in\mathcal{A}$ is the current action
		\item A \textcolor{RoyalBlue}{transition function} $\mathcal{P}:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\rightarrow[0,1]$
		\item A \textcolor{RoyalBlue}{reward function} $\Re:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\rightarrow \mathbb{R}$ which returns $r_t$
	\end{itemize}

	\begin{block}{Markov Decision Processes (MDPs)}
		These components allows us to define a \textcolor{Maroon}{Markov Decision Process} 
		\centering $\mathcal{M}=\langle \mathcal{S},\mathcal{A},\mathcal{P},r\rangle$.
	\end{block}

\end{frame}


%============================================================================

%============================================================================
\begin{frame}{Markov Decision Processes}
	\section{Mathematical Framework}
	What is so special about MDPs?
	\begin{block}{Markov Property}
		In a Markovian \textcolor{RoyalBlue}{environment} the conditional distribution of the next state of the process only depends from the current state of the process. 		\centering  	
		\begin{align*}
		p(s_{t+1}|s_t, a_t, s_{t-1}, a_{t-1}, \ldots) = p(s_{t+1} | s_t, a_t).
		\end{align*}
	\end{block}

	Interestingly, the same property also holds for the \textcolor{RoyalBlue}{reward} that the agent will get: 
	\begin{align*}
			p(r_t| s_t, a_t, \ldots, s_1, a_1) = p(r_t|s_t,a_t).
	\end{align*}
\end{frame}

\begin{frame}{Markov Decision Processes}
	\textcolor{Maroon}{How} does an agent \textcolor{RoyalBlue}{interact} with its environment?
	
	\begin{block}{Policy $\pi$}		
		Through a probability distribution over $a \in \mathcal{A}(s)$ for each $s \in \mathcal{S}$:
		\centering
		\begin{align*}
			\pi(a|s) = \text{Pr}\; \{a_t = a | s_t = s\}, \; \text{for all}\; s \in \mathcal{S}\; \text{and}\; a\ \in \mathcal{A}. 
		\end{align*}

	\end{block}
	
	which more simply can be seen as a \textcolor{RoyalBlue}{mapping} from states to actions that determines the behaviour of the agent: \\ 
	\centering $\pi: \mathcal{S}\rightarrow \mathcal{A}$ \\	
\end{frame}

\begin{frame}{Episodes}
	Once we have policy $\pi$ we can let the agent interact with the environment, which results in an \textcolor{RoyalBlue}{episode}:
	\centering
	\begin{align*}
		\langle(s_t,a_t,r_t,s_{t+1})\rangle, t=0,\ldots,T-1
	\end{align*}
	where $T$ is a random variable defining the \textcolor{RoyalBlue}{length} of the episode \\

	\bigskip
	One $\langle s_t,a_t,r_t,s_{t+1} \rangle$ is called a \textcolor{RoyalBlue}{trajectory} $\tau$
\end{frame}



\begin{frame}{Goals and Returns}
	\textcolor{Maroon}{Why} do we want an agent to interact with the environment?
	\begin{itemize}
		\item We would like it to \textcolor{RoyalBlue}{master} a certain task
		\item Ideally it could \textcolor{RoyalBlue}{discover} solutions that are unknown to us
		\item \textcolor{RoyalBlue}{Biologically plausible} form of learning
	\end{itemize}

	How do we formalize this mathematically?

	\begin{block}{Goal}
		In its easiest form we can define such goal as \textcolor{RoyalBlue}{maximizing} the sequence of $r_t$ returned by $\Re$
		\begin{align*}
			G_t = r_t + r_{t+1} + r_{t+2}, \ldots, r_T.
		\end{align*}
	\end{block}

\end{frame}

\begin{frame}{Goals and Returns}
	\textcolor{Maroon}{However} the definition of $G_t$ has some limitations:
	\begin{itemize}
		\item Assumes that episodes are finite 
		\item Many real world applications are instead continuous, therefore $T=\infty$
		\item As a result $G_t$ can be infinite as well
	\end{itemize}

	\bigskip

	We need a slightly more complex definition of $G_t$ based on the concept of \textcolor{RoyalBlue}{discounting} modeled by $\gamma$  

\end{frame}

\begin{frame}{Goals and Returns}
	\begin{block}{Discounted Return}
  	 $\gamma$ allows us to define the notion of \textcolor{RoyalBlue}{discounted cumulative reward}	
		\begin{align*}
			G_t & = r_t+\gamma r_{t+1}, \gamma^{2} r_{t+2} + ... \\
				& = \sum_{k=0}^{\infty}\gamma^{k} r_{t+k+1}.
		\end{align*}

	\end{block}

	\begin{itemize}
		\item $\gamma$ determines the value of future rewards as $0\leq\gamma\leq1$
		\item Makes $G_t$ finite as long as $\gamma < 1$ and $r_t$ is constant
	\end{itemize}
	
	\centering
	\begin{align*}
		G_t = \sum_{k=0}^{\infty}\gamma^{k} = \frac{1}{1-\gamma}
	\end{align*}

\end{frame}


%============================================================================

%============================================================================
\begin{frame}{Value Functions}
	\section{Value Functions}

	One of the most \textcolor{Maroon}{concepts} of Reinforcement Learning: \textcolor{RoyalBlue}{value}
	\begin{itemize}
		\item Directly connected to the notion of $G_t$
		\item We can define the value of a state $s$, of an action $a$, and even of a policy $\pi$
		\item Allows the introduction of \textcolor{RoyalBlue}{value functions}
			\begin{itemize}
				\item state-value function $V(s)$
				\item state-action value function $Q(s,a)$
			\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}{Value Functions}
	\begin{block}{The state-value function $V^{\pi}(s)$}
		\begin{align*}
			V^{\pi}(s) & = \mathds{E} \bigg[G_t \: \big| \: s_t=s,\pi \bigg] \\ 
				& = \mathds{E} \bigg[\sum_{k}^{\infty}\gamma^{k} r_{t+k+1} \: \bigg| \: s_t=s,\pi \bigg]
		\end{align*}
	\end{block}

	\begin{itemize}
		\item The \textcolor{RoyalBlue}{easiest} value function to learn
		\item Intuitively it tells us \textit{``how good/bad"} every state $s$ visited by policy $\pi$ is
		\item This \textit{``goodness"} is expressed with respect to $G_t$
	\end{itemize}

\end{frame}

\begin{frame}{Value Functions}
	The state-value function is only conditioned on the states that are being visited
	
	\begin{block}{The state-action value function $Q^{\pi}(s,a)$}
		\begin{align*}
		Q^{\pi}(s,a) & = \mathds{E} \bigg[G_t \: \big| \: s_t=s, \: a_t=a,  \pi \bigg] \\ 
			& = \mathds{E} \bigg[\sum_{k}^{\infty}\gamma^{k} r_{t+k+1} \: \bigg| \: s_t=s, \: a_t=a, \pi \bigg]
		\end{align*}
	\end{block}

	\begin{itemize}
		\item Is much more \textcolor{RoyalBlue}{informative} compared to $V^{\pi}(s)$
		\item Intuitively tells us \textit{how good/bad} taking action $a$ in state $s$ is
		\item Plays a crucial role in the \textcolor{RoyalBlue}{development} of Reinforcement Learning algorithms
	\end{itemize}

\end{frame}

\begin{frame}{Value Functions}
	An interesting property of these value functions is that they satisfy a \textcolor{RoyalBlue}{recursive equality}

		\begin{align*}
			V^{\pi}(s) & = \mathds{E} \bigg[\sum_{k}^{\infty}\gamma^{k} r_{t+k+1} \: \bigg| \: s_t=s,\pi \bigg] \\
			   & = \sum_a \pi(s,a) \sum_{s_{t+1}} p(s_{t+1}|s,a)\big[\Re(s_t, a, s_{t+1}) + \gamma V^{\pi}(s_{t+1}) \big] 
		\end{align*}
\end{frame}


\begin{frame}{Value Functions}
	An interesting property of these value functions is that they satisfy a \textcolor{RoyalBlue}{recursive equality}
		\begin{align*}
			\textcolor{blue}{V^{\pi}(s)} & = \mathds{E} \bigg[\sum_{k}^{\infty}\gamma^{k} r_{t+k+1} \: \bigg| \: s_t=s,\pi \bigg] \\
			   & = \sum_a \pi(s,a) \sum_{s_{t+1}} p(s_{t+1}|s,a)\big[\Re(s_t, a, s_{t+1}) + \gamma \textcolor{blue}{V^{\pi}(s_{t+1}}) \big] 
		\end{align*}
\end{frame}


\begin{frame}{Value Functions}
	An interesting property of these value functions is that they satisfy a \textcolor{RoyalBlue}{recursive equality}
		\begin{align*}
		\textcolor{blue}{V^{\pi}(s)} & = \mathds{E} \bigg[\sum_{k}^{\infty}\gamma^{k} r_{t+k+1} \: \bigg| \: s_t=s,\pi \bigg] \\
			   & = \sum_a \pi(s,a) \sum_{s_{t+1}} p(s_{t+1}|s,a)\big[\Re(s_t, a, s_{t+1}) + \gamma \textcolor{blue}{V^{\pi}(s_{t+1}}) \big] 
		\end{align*}

	which is \textcolor{Maroon}{key} for the creation of Reinforcement Learning algorithms (Lectures 3 and 4)!
\end{frame}


\begin{frame}{Optimal Value Functions}
	Solving a Reinforcement Learning problem intuitively means finding a policy $\pi$ that achieves \textcolor{RoyalBlue}{a lot of reward}:
	\begin{itemize}
		\item What makes a certain policy $\pi$ \textcolor{RoyalBlue}{better} than another policy $\pi^{'}$? 
		\item How can we \textcolor{RoyalBlue}{rank} different policies?
	\end{itemize}
	
	We can answer these questions thanks to the $V(s)$ and the $Q(s,a)$ functions:

	\begin{block}{Optimal Policy $\pi^*$}
		There is always one policy that is better or equal than all other policies
	\begin{align*}
		\pi \geq \pi^{'}\: \text{iff}\: V^{\pi}(s) \geq V^{\pi^{'}}(s) \: \text{for all}\: s\in\mathcal{S}
	\end{align*}
		The best possible policy is the \textcolor{RoyalBlue}{optimal policy} $\pi^{*}$
	\end{block}
\end{frame}


\begin{frame}{Optimal Value Functions}
	\begin{itemize}
		\item How do we \textcolor{RoyalBlue}{find} the optimal policy $\pi^{*}$?
		\item By \textcolor{RoyalBlue}{maximizing} the state-value and state-action value functions results in the \textcolor{RoyalBlue}{optimal value functions}
	\end{itemize}
	\begin{multicols}{2}
		\begin{equation*}
			V^{*}(s)=\underset{\pi}{\max}\:V^{\pi}(s)
		\end{equation*}\break
		\begin{equation*}
			Q^{*}(s,a)= \underset{\pi}{\max}\:Q^{\pi}(s,a)
		\end{equation*}
	\end{multicols}

	which, if expressed in a recursive form, result in the \textcolor{Maroon}{Bellman optimality equations} (for the coming lectures!) 

\end{frame}



%============================================================================


\begin{frame}{Final Slide!}
	We aim to teach an agent how to \textcolor{RoyalBlue}{interact} with its environment:
	\begin{takeaway}{Lecture Takeaway}
		\begin{enumerate}
			\item The environment is modelled as a Markov Decision Process $\mathcal{M}\langle \mathcal{S}, \mathcal{A}, \mathcal{P}, r \rangle$
			\item The interaction is governed by the agent's policy $\pi$
			\item The goal of the agent is measured wrt discounted cumulative reward $G_t$
			\item $G_t$ allows us to quantify how good a certain state $s$ is, and how good a certain action $a$ in a certain state is: $V^\pi(s)$ and $Q^\pi(s,a)$ 
		\end{enumerate}
	\end{takeaway}
\end{frame}

\end{document}
