\documentclass{beamer}

\usepackage[english]{babel}
\usepackage{xcolor}
\usepackage{xmpmulti}
\usepackage{amsmath}
\usepackage{dsfont}
\usepackage{tikz}
\usepackage{eucal}
\usetikzlibrary{positioning,angles,quotes}
\usepackage{url}
\usepackage{graphicx}
\usepackage{cmbright}


\usetikzlibrary{pgfplots.groupplots,arrows.meta,shadows,positioning,angles,quotes}
\usetikzlibrary{matrix,chains,positioning,decorations.pathreplacing,arrows}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{positioning}
\usepackage{pgfplots}

%\input{epgfplotslibrary{groupplots}
\usetikzlibrary{pgfplots.groupplots,arrows.meta,shadows,positioning,angles,quotes}
\usetikzlibrary{matrix,chains,positioning,decorations.pathreplacing,arrows}



\DeclareMathOperator*{\argmax}{arg\,max}
\def\checkmark{\tikz\fill[scale=0.4](0,.35) -- (.25,0) -- (1,.7) -- (.25,.15) -- cycle;} 

\definecolor{Maroon}{cmyk}{0, 0.87, 0.68, 0.32}
\definecolor{RoyalBlue}{cmyk}{1, 0.50, 0, 0}


%%%%%% THE FOLLOWING FILE CONTAINS THE STYLE DEFINITIONS %%%%%%
\input{header.tex}
%%%%%%

%%%%%% TITLE, AUTHOR, DATE DEFINITIONS %%%%%%
\title{Foundations of Reinforcement Learning}
\subtitle{From Markov Decision Processes to Optimal Value Functions}
\author{Matthia Sabatelli}


\date{\today}
%%%%%%

\setbeamertemplate{footline}[frame number]{}

\begin{document}

\frame{\titlepage} 

\frame{\frametitle{Today's Agenda}\tableofcontents}

%============================================================================
\begin{frame}{Course Information}
	\section{Course Information}

	\begin{itemize}
		\item Coordinator: Matthia Sabatelli
		\item Lecturers: Matthia Sabatelli (\textcolor{RoyalBlue}{m.sabatelli@rug.nl}) and Nicole Orzan (\textcolor{RoyalBlue}{n.orzan@rug.nl})
		\item Classroom: TBD
		\item Theoretical Lectures: Monday morning from 9:00-11:00
		\item Computer Labs: Monday afternoon from 15:00-17:00
	\end{itemize}

\end{frame}


\begin{frame}{Course Information}

	\begin{itemize}
		\item Lecture 1: Foundations of Reinforcement Learning (Matthia)
		\item Lecture 2: Exploration and Bandit Problems (Nicole)
		\item Lecture 3: Dynamic Programming (Nicole)
		\item Lecture 4: Model-Free Reinforcement Learning (Matthia)
		\item Lecture 5: Function Approximators (Matthia) 
		\item Lecture 6: Beyond Model-Free Reinforcement Learning (Matthia)
		\item Lecture 7: \textit{What does it mean to do research in RL?} (Matthia \& Nicole)
	\end{itemize}

\end{frame}

\begin{frame}{Course Information}

	All \textcolor{RoyalBlue}{course material} will be made available

	\begin{itemize}
		\item Nestor
		\item Github: \url{https://github.com/paintception/reinforcement-learning-practical}
	\end{itemize}

\end{frame}

\begin{frame}{Course Information}
	\textcolor{RoyalBlue}{Textbook}: Reinforcement Learning: An Introduction by Sutton \& Barto
\end{frame}

\begin{frame}{Course Information}
	Final course \textcolor{RoyalBlue}{assessement}:
	\begin{itemize}
		\item There is \textbf{no exam}
		\item Students should handle in three deliverables:
			\begin{enumerate}
				\item Assignment 1: $25\%$ of the grade (coding)
				\item Assignment 2: $25\%$ of the grade (mathematics)
				\item Report: $50\%$ of the grade (final project)
			\end{enumerate}
		\item Students can work alone or in groups of a maximum of \textbf{2} people 
	\end{itemize}

\end{frame}

%============================================================================






%============================================================================
\begin{frame}{Reinforcement Learning}
	\section{What is Reinforcement Learning}
	
	Machine Learning is typically divided into three branches:

	\begin{enumerate}
		\item Supervised Learning: learning from \textcolor{RoyalBlue}{labeled} data
		\item Unsupervised Learning: learning from \textcolor{RoyalBlue}{unlabeled} data
		\item Reinforcement Learning: learning from \textcolor{RoyalBlue}{experience}
	\end{enumerate}

\end{frame}


\begin{frame}{Reinforcement Learning}
	A reminder of \textcolor{RoyalBlue}{supervised learning}:
	\begin{itemize}
		\item input space: $\mathcal{X}$
		\item output space: $\mathcal{Y}$
		\item probability distribution $p(x,y)$
	\end{itemize}

	\begin{block}{The goal}
		We want to build a function $f:\mathcal{X}\rightarrow\mathcal{Y}$ that minimizes the \textcolor{Maroon}{expectation} of a given loss $\ell$
		\begin{align*}
			\mathds{E}_{(x,y)\sim p(x,y)}\{\ell(y,f(x)) \}
		\end{align*}
		through leaning samples $LS=\{(x_i,y_i)|i=1,...,N\}$ of input-ouput pairs drawn from $p(x,y)$.
	\end{block}

\end{frame}

\begin{frame}{Reinforcement Learning}
	Supervised learning is therefore:
	\begin{itemize}
		\item a learning paradigm which is \textcolor{Maroon}{static}
		\item \textcolor{Maroon}{no interaction} happens between the learner and $p(x,y)$
		\item Assumes we have access to a \textcolor{Maroon}{knowledgable supervisor}
	\end{itemize}

	\bigskip
	In Reinforcement Learning however ...
\end{frame}

\end{frame}{Reinforcement Learning}
	\begin{itemize}
		\item We would like to learn how to \textcolor{RoyalBlue}{interact} with an environment 
		\item We do not assume any sort of supervision but only a \textcolor{RoyalBlue}{reward} signal
		\item The component of \textcolor{RoyalBlue}{time} plays a crucial role
		\item The learning process is therefore \textcolor{RoyalBlue}{dynamic} 
	\end{itemize}
\begin{frame}

\end{frame}


%============================================================================

%============================================================================
\begin{frame}{Markov Decision Processes}
	\section{Markov Decision Processes}

	


\end{frame}


\begin{frame}{Reinforcement Learning}
	\input{./images/rl_loop.tex}

\end{frame}



%============================================================================

%============================================================================
\begin{frame}{Value Function}
	\section{Value Functions}

\end{frame}
%============================================================================




\end{document}
