\documentclass{beamer}


\usepackage[english]{babel}
\usepackage{xcolor}
\usepackage{xmpmulti}
\usepackage{amsmath}
\usepackage{dsfont}
\usepackage{multicol}
\usepackage{tikz}
\usepackage{eucal}
\usetikzlibrary{positioning,angles,quotes}
\usepackage{url}
\usepackage{graphicx}
\usepackage{cmbright}
\usepackage{framed}
\usepackage{algorithm}
%\usepackage[]{algorithm2e}
%\usepackage{algpseudocode}

\usepackage[noend]{algpseudocode}

\usetikzlibrary{pgfplots.groupplots,arrows.meta,shadows,positioning,angles,quotes}
\usetikzlibrary{matrix,chains,positioning,decorations.pathreplacing,arrows}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{positioning}
\usepackage{pgfplots}



%\input{epgfplotslibrary{groupplots}
\usetikzlibrary{pgfplots.groupplots,arrows.meta,shadows,positioning,angles,quotes}
\usetikzlibrary{matrix,chains,positioning,decorations.pathreplacing,arrows}



\DeclareMathOperator*{\argmax}{arg\,max}
\def\checkmark{\tikz\fill[scale=0.4](0,.35) -- (.25,0) -- (1,.7) -- (.25,.15) -- cycle;} 

\definecolor{Maroon}{cmyk}{0, 0.87, 0.68, 0.32}
\definecolor{RoyalBlue}{cmyk}{1, 0.50, 0, 0}
\definecolor{skymagenta}{rgb}{0.81, 0.44, 0.69}

\newenvironment{takeaway}[1]{%
	\definecolor{shadecolor}{gray}{0.9}%
		\begin{shaded}{\color{skymagenta}\noindent\textsc{#1}}\\%
		}{%
		\end{shaded}%
}



%%%%%% THE FOLLOWING FILE CONTAINS THE STYLE DEFINITIONS %%%%%%
\input{header.tex}
%%%%%%

%%%%%% TITLE, AUTHOR, DATE DEFINITIONS %%%%%%
\title{Dynamic Programming}
\subtitle{Finding the Optimal Policy given a model of the environment}
\author{Nicole Orzan}


\date{\today}
%%%%%%

\setbeamertemplate{footline}[frame number]{}

\begin{document}

\frame{\titlepage} 

%============================================================================

\begin{frame}{Recap}
	The mathematical framework of Reinforcement Learning:
	\begin{itemize}
		\item A set of possible \textcolor{RoyalBlue}{states} $\mathcal{S}$ where $s_t\in\mathcal{S}$ is the current state
		\item A set of possible \textcolor{RoyalBlue}{actions} $\mathcal{A}$ where $a_t\in\mathcal{A}$ is the current action
		\item A \textcolor{RoyalBlue}{transition function} $\mathcal{P}:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\rightarrow[0,1]$
		\item A \textcolor{RoyalBlue}{reward function} $\Re:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\rightarrow \mathbb{R}$ which returns $r_t$
	\end{itemize}

	\vspace{1mm}
	\begin{block}{Markov Decision Processes (MDPs)}
		These components allows us to define a \textcolor{Maroon}{Markov Decision Process} 
		\centering $\mathcal{M}=\langle \mathcal{S},\mathcal{A},\mathcal{P},r\rangle$.
	\end{block}

\end{frame}


\begin{frame}{Recap}

\begin{block}{Key Concepts}
\begin{itemize}
	\item The agent aims to learn a \textcolor{RoyalBlue}{policy} $\pi: \mathcal{S}\rightarrow \mathcal{A}$
	\item The goal of the agent is to maximize the (discounted) \textcolor{RoyalBlue}{cumulative reward} $G_t = \sum_{k=0}^{\infty}\gamma^{k} r_{t+k+1}$.
	\item We can find the optimal policy by making use of the \textcolor{RoyalBlue}{value functions} $V^\pi(s)$ and $Q^\pi(s,a)$ 
	\item We saw how to find an optimal policy in the specific case where \textcolor{RoyalBlue}{one single state} is available
\end{itemize}
\end{block}

\vspace{2mm}

	Today we will consider the full Reinforcement Learning problem with its \textcolor{RoyalBlue}{sequential nature}.
	
\end{frame}

\frame{\frametitle{Today's Agenda}\tableofcontents}

\begin{frame}{Change in Notation}

\begin{block}{Attention!}
\begin{normalsize}
So far we denoted the reward function as \textcolor{Maroon}{$\Re(s, a, s_{t+1})$}.
\vspace{1mm}

For the sake of simplicity, hereafter we will denote it as \textcolor{Maroon}{$r$}.
\end{normalsize}
\end{block}
\end{frame}


%============================================================================
\begin{frame}{Value Functions}
\section{Value Functions}

The \textcolor{RoyalBlue}{state-value function} $V^{\pi}(s)$:
\begin{align*}
	V^{\pi}(s) & = \mathds{E} \bigg[G_t \: \big| \: s_t=s,\pi \bigg] \\ 
		& = \mathds{E} \bigg[\sum_{k=0}^{\infty}\gamma^{k} r_{t+k+1} \: \bigg| \: s_t=s,\pi \bigg]
\end{align*}

The \textcolor{RoyalBlue}{state-action value function} $Q^{\pi}(s,a)$:
\begin{align*}
	Q^{\pi}(s,a) & = \mathds{E} \bigg[G_t \: \big| \: s_t=s, \: a_t=a,  \pi \bigg] \\ 
		& = \mathds{E} \bigg[\sum_{k=0}^{\infty}\gamma^{k} r_{t+k+1} \: \bigg| \: s_t=s, \: a_t=a, \pi \bigg]
\end{align*}
		
\end{frame}


%============================================================================
\begin{frame}{Bellman Equations}


The \textcolor{RoyalBlue}{state-value function} $V^{\pi}(s)$:
\begin{align*}
V^{\pi}(s) & = \mathds{E} \bigg[\sum_{k=0}^{\infty}\gamma^{k} r_{t+k+1} \: \bigg| \: s_t=s,\pi \bigg] \\
   & = \sum_a \pi(a|s) \sum_{s_{t+1}} p(s_{t+1}|s,a)\big[r + \gamma V^{\pi}(s_{t+1}) \big] 
\end{align*}

The \textcolor{RoyalBlue}{state-action value function} $Q^{\pi}(s,a)$:
\begin{align*}
	Q^{\pi}(s,a) & = \mathds{E} \bigg[\sum_{k}^{\infty}\gamma^{k} r_{t+k+1} \: \bigg| \: s_t=s, a_t=a,\pi \bigg] \\
	& = \sum_{s_{t+1}} p(s_{t+1}|s,a)\big[r + \gamma V^{\pi}(s_{t+1}) \big] \\
	& = \sum_{s_{t+1}} p(s_{t+1}|s,a)\big[r + \gamma \sum_{a_{t+1}} \pi(a_{t+1}|s_{t+1}) Q^{\pi}(s_{t+1}, a_{t+1}) \big]
\end{align*}
		
\end{frame}



%============================================================================
\begin{frame}{Optimal Policy}

The value functions allow us to define a partial ordering over policies. 
	\begin{align*}
		\pi \geq \pi^{\prime}\: \text{iff}\: V^{\pi}(s) \geq V^{\pi^{'}}(s) \: \text{for all}\: s\in\mathcal{S}
	\end{align*}
	
	We can find the optimal policies by \textcolor{RoyalBlue}{maximizing} the state-value and state-action value functions results in the \textcolor{RoyalBlue}{optimal value functions}.
	\begin{align*}
	V^{*}(s)=\underset{\pi}{\max}\:V^{\pi}(s) \hspace{18mm} Q^{*}(s,a)= \underset{\pi}{\max}\:Q^{\pi}(s,a)
	\end{align*}

There is always at least one policy better than any other one: $\pi^{*}$.

\end{frame}

%============================================================================
\begin{frame}{Bellmann Optimality Equations}



   \begin{align*}
	V^{*}(s)& = \mathds{E} \bigg[\sum_{k=0}^{\infty}\gamma^{k} r_{t+k+1} \: \bigg| \: s_t=s,\pi \bigg] \\
		   & = \max_a \sum_{s_{t+1}} p(s_{t+1}|s,a)\big[r + \gamma V^{*}(s_{t+1}) \big] 
	\end{align*}
	
	   \begin{align*}
	Q^{*}(s,a)& = \mathds{E} \bigg[\sum_{k=0}^{\infty}\gamma^{k} r_{t+k+1} \: \bigg| \: s_t=s, a_t=a,\pi \bigg] \\
		   & = \sum_{s_{t+1}} p(s_{t+1}|s,a)\big[r + \gamma \max_{a_{t+1}}Q^{*}(s_{t+1}, a_{t+1}) \big] 
	\end{align*}

\end{frame}



\begin{frame}{Dynamic Programming}
\section{Dynamic Programming}

With the term Dynamic Programming we refer to a set of algorithms that, \textcolor{Maroon}{given a model of the environment} $p(s_{t+1}| s, a)$, allow us to find the optimal policies.

\vspace{3mm}

\begin{block}{Key Idea}
\begin{normalsize}
The key idea of Dynamic Programming is the \textcolor{RoyalBlue}{use of the Bellman equations} to organize the search for good policies.
\end{normalsize}
\end{block}
\end{frame}

\begin{frame}{Dynamic Programming}

The goal of Reinforcement Learning is to find the optimal policy $\pi(a|s)$ for each $s \in \mathcal{S}$ for a given problem. 

This task is called \textcolor{RoyalBlue}{Control}.

\vspace{3mm}

Often, to be able to solve the problem of Control, it is necessary to evaluate the goodness of a policy.

This task is called \textcolor{RoyalBlue}{Policy Evaluation}.

\end{frame}



\begin{frame}{Control}
\section{Control}

Improving a policy

\begin{figure}[t]
\includegraphics[width=8cm]{./images/pi1.png}
\centering
\end{figure}

\end{frame}


\begin{frame}{Control}

Improving a policy

\begin{figure}[t]
\includegraphics[width=8cm]{./images/pi2.png}
\centering
\end{figure}

\end{frame}

\begin{frame}{Control}

Improving a policy

\begin{figure}[t]
\includegraphics[width=8cm]{./images/pi3.png}
\centering
\end{figure}

\end{frame}


%============================================================================
\begin{frame}{Policy Evaluation}
\section{Policy Evaluation}

\textcolor{RoyalBlue}{Evaluating} a policy $\pi$ means computing its state-value function (or state-action value function).

\vspace{2mm}

If the environment dynamic is completely known, this problem is reduced to finding the solution to a set of $\mathcal{|S|}$ linear equations:



\begin{align*}
    V^{\pi}(s)& = \mathds{E} \bigg[\sum_{k=0}^{\infty}\gamma^{k} r_{t+k+1} \: \bigg| \: s_t=s,\pi \bigg] \\
		   & = \sum_a \pi(a|s) \sum_{s_{t+1}} p(s_{t+1}|s,a)\big[r + \gamma V^{\pi}({s_{t+1}}) \big] 
\end{align*}

\end{frame}


%============================================================================
\begin{frame}{Iterative Policy Evaluation}

Since the above computation is complex, we can compute the value function for a given policy \textcolor{RoyalBlue}{iteratively}:
\begin{itemize}
    \item We choose $V^0$ arbitrarily
    \item We compute a sequence of improved approximations $V^1, V^2, V^3 ...$  by applying the Bellman equation to the previous estimate:
\end{itemize}

\begin{align*}
    &V^{k+1}(s) = \mathds{E} \bigg[\sum_{k=0}^{\infty}\gamma^{k} r_{t+k+1} \: \bigg| \: s_t=s,\pi \bigg] \\
		   & = \sum_a \pi(a|s) \sum_{s_{t+1}} p(s_{t+1}|s,a)\big[r + \gamma V^{k}(s_{t+1}) \big] \; \text{for each } s \in S
\end{align*}

The sequence $\{V_k\}$ is proven to converge to $V^{\pi}$ as $k \rightarrow \infty $.

\end{frame}



\begin{frame}{Gridworld}

\begin{itemize}
    \item $\mathcal{S}$: 16 states, with 2 terminal ones (0 and 15)
    \item $\mathcal{A}$: Four possible actions: $\uparrow$ (up), $\downarrow$ (down), $\leftarrow$ (left), $\rightarrow$ (right)
    \item the function $p(s_{t+1}|s,a)$ is deterministic
\item $\Re(s,a)$: $r_t = -1$ for every step
\end{itemize}

\begin{figure}[t]
\includegraphics[width=4.5cm]{./images/GridWorld.png}
\centering
\end{figure}


\end{frame}


\begin{frame}{Iterative Policy Evaluation}

\begin{footnotesize}

Let's evaluate the \textcolor{RoyalBlue}{uniform random policy}, which has probability $0.25$ of taking each of the 4 possible actions. $\gamma = 1$.

\begin{align*}
    V^{k+1}(s) := \sum_a \textcolor{blue}{\pi(a|s)} \sum_{s_{t+1},r} p(s_{t+1}|s,a)\Big[\textcolor{orange}{r+\gamma\:V^k(s_{t+1})}\Big]
\end{align*}
\begin{align*}
	V^1(s = 1) = \; & \textcolor{blue}{\pi(\uparrow|s=1)}[\;\textcolor{orange}{-1 + V^0(s=1)}\;] + \textcolor{blue}{\pi(\downarrow|s=1)}[\; \textcolor{orange}{-1+ V^0(s=5)}\; ] \; + \\
	&\textcolor{blue}{\pi(\leftarrow|s=1)}[\;\textcolor{orange}{-1+ V^0(s=0)}\;] + \textcolor{blue}{\pi(\rightarrow|s=1)}[\;\textcolor{orange}{-1+ V^0(s=2)}\;]
\end{align*}

\end{footnotesize}

\begin{figure}[t]
\includegraphics[width=9.4cm]{./images/step00_bis2.png}
\centering
\end{figure}

\end{frame}

\begin{frame}{Iterative Policy Evaluation}


\begin{footnotesize}
Let's evaluate the \textcolor{RoyalBlue}{uniform random policy}, which has probability $0.25$ of taking each of the 4 possible actions. $\gamma = 1$.

\begin{align*}
    V^{k+1}(s) := \sum_a \textcolor{blue}{\pi(a|s)} \sum_{s_{t+1},r} p(s_{t+1}|s,a)\Big[\textcolor{orange}{r+\gamma\:V^k(s_{t+1})}\Big]
\end{align*}

\begin{align*}
    V^1(s = 1) = \; &\textcolor{blue}{0.25} \; [\;\textcolor{orange}{-1 + 0}\; ] + \textcolor{blue}{0.25} \; [\;\textcolor{orange}{-1+0}\; ]\; + \\
    &\textcolor{blue}{0.25} \; [\;\textcolor{orange}{-1+0}\; ] + \textcolor{blue}{0.25} \; [\;\textcolor{orange}{-1+0}\; ] = -1
\end{align*}

\end{footnotesize}

\begin{figure}[t]
\includegraphics[width=9.4cm]{./images/step00.png}
\centering
\end{figure}

\end{frame}




\begin{frame}{Iterative Policy Evaluation}


\begin{footnotesize}
Let's evaluate the \textcolor{RoyalBlue}{uniform random policy}, which has probability $0.25$ of taking each of the 4 possible actions.  $\gamma = 1$.

\begin{align*}
    V^{k+1}(s) := \sum_a \textcolor{blue}{\pi(a|s)} \sum_{s_{t+1}} \sum_r p(s_{t+1}|s,a)\Big[\textcolor{orange}{r+\gamma\:V^k(s_{t+1})}\Big]
\end{align*}

\begin{align*}
    V^1(s = 1) = \; &\textcolor{blue}{0.25} \; [\;\textcolor{orange}{-1 + 0}\; ] + \textcolor{blue}{0.25} \; [\;\textcolor{orange}{-1+0}\; ]\; + \\
    &\textcolor{blue}{0.25} \; [\;\textcolor{orange}{-1+0}\; ] + \textcolor{blue}{0.25} \; [\;\textcolor{orange}{-1+0}\; ] = -1
\end{align*}


\end{footnotesize}

\begin{figure}[t]
\includegraphics[width=9.4cm]{./images/step0.png}
\centering
\end{figure}

\end{frame}



\begin{frame}{Iterative Policy Evaluation}

\begin{small}
\begin{align*}
    V^{k+1}(s) := \sum_a \textcolor{blue}{\pi(a|s)} \sum_{s_{t+1}} \sum_r p(s_{t+1}|s,a)\Big[\textcolor{orange}{r+\gamma\:V^k(s_{t+1})}\Big]
\end{align*}

\end{small}
\begin{figure}[t]
\includegraphics[width=9.7cm]{./images/stepn.png}
\centering
\end{figure}

\end{frame}



\begin{frame}{Iterative Policy Evaluation}


\begin{algorithm}[H]
\caption{Iterative Policy Evaluation}

%\SetAlgoLined

\vspace{1mm}
Input the policy to evaluate $\pi$

\vspace{0.5mm}

Define threshold for accuracy $\theta$

\vspace{0.5mm}

Initialize $V(s)$, for all $s \in \mathcal{S}$ arbitrarily, except V(terminal) = 0

\vspace{4mm}

$\Delta \leftarrow 0$
 

 \textbf{\text{while}} \text{True} 

 \hspace{5mm}  \textbf{\text{for}} \text{each} $s \in \mathcal{S}$
   
   \vspace{0.5mm}
   
   \hspace{15mm} $v \leftarrow V(s)$
   
   \vspace{0.7mm}
   
   \hspace{15mm} $V(s) \leftarrow \sum_{a} \pi(a|s) \sum_{s^{\prime}, r} p(s_{t+1}| s, a)[r + \gamma V(s_{t+1})]$
   
   \vspace{0.7mm}
   
   \hspace{15mm} $\Delta \leftarrow \max(\Delta, |v - V(s)|)$
   
   \vspace{0.5mm}
   
   \hspace{5mm} if $\Delta < \theta$ return V
   \vspace{1mm}
   

\end{algorithm}

\end{frame}

%============================================================================
\begin{frame}{Policy Improvement}

What happens if, instead of following policy $\pi$ in state $s$, we decide to take action $a$?

\begin{align*}
    &Q^{\pi}(s,a) = \mathds{E} \bigg[\sum_{k}^{\infty}\gamma^{k} r_{t+k+1} \: \bigg| \: s_t=s, a_t=a,\pi \bigg] \\
		   & =  \sum_{s_{t+1}} p(s_{t+1}|s,a)\big[r + \gamma V^{\pi}(s_{t+1}) \big]
\end{align*}

If this value is greater than $V^{\pi}(s)$, we should definitely take action $a$.

\end{frame}

\begin{frame}{Policy Improvement}


\begin{block}{Policy Improvement Theorem}
Given any pair of deterministic policies $\pi$ and $\pi^{\prime}$ such that, for all $s \in \mathcal{S}$:
\begin{align*}
Q^{\pi}(s, \pi^{\prime}(s)) \geq V^{\pi}(s)
\end{align*}
Then policy $\pi^{\prime}$ is as good as or better than $\pi$:
\begin{align*}
    V^{\pi^{\prime}}(s) \geq V^{\pi}(s)
\end{align*}
\end{block}

\hspace{1mm}

An easy way to find a new policy $\pi^{\prime}$ which is always better than $\pi$ is:
\begin{align*}
    \pi^{\prime} = \argmax_{a \in A} Q^{\pi}(s,a)
\end{align*}

\end{frame}

\begin{frame}{Policy Iteration}
\section{Iterative Dynamic Programming Algorithms}

\begin{align*}
\pi_0 \; \overset{E}{\longrightarrow} \; V^{\pi_0} \overset{I}\longrightarrow \pi_1 \overset{E}\longrightarrow V^{\pi_1} \overset{I}\longrightarrow . . . \overset{I}\longrightarrow \pi_{*} \overset{E}\longrightarrow V^{\pi_*}
\end{align*}

Since a finite MDP has only a finite number of policies, this process must converge to an \textcolor{RoyalBlue}{optimal policy} in a finite number of iterations.

\end{frame}

\begin{frame}{Policy Iteration}

\begin{algorithm}[H]
\caption{Policy Iteration for estimating $\pi \sim \pi^{*}$}

\vspace{1mm}

%\SetAlgoLined
1) \textbf{\text{Initialization}}

Initialize $V(s)$ and $\pi(s)$ \text{ arbitrarily for all } $s \in \mathcal{S}$
\vspace{3mm}


2) \textbf{\text{Policy Evaluation Step}} (Algorithm 1)
\vspace{3mm}

3) \textbf{\text{Policy Improvement Step:}}

\vspace{0.5mm}

\hspace{6mm}policy-stable $\leftarrow$ true 

\vspace{0.5mm}

 \hspace{5mm} \textbf{\text{for}} \text{each} $s \in \mathcal{S}$ \textbf{\text{do}} 
 
 \vspace{0.4mm}
 
 \hspace{11mm} old-action $\leftarrow \pi(s)$
 
 \vspace{0.5mm}
 
 \hspace{11mm} $\pi(s) \leftarrow \argmax_a \sum_{s_{t+1}} p(s_{t+1}|s,a) [r + \gamma V(s_{t+1})]$
 
 \vspace{0.5mm}
 
 \hspace{11mm} if old\text{-}action $\neq \pi(s)$, \text{policy-stable} $\leftarrow$ false
 
 \vspace{0.5mm}
 
 \hspace{6mm}if policy-stable = true then return $\pi \sim \pi^{*}$ and $V \sim V^{*}$
 
 \hspace{4.7mm} else go to 2
 

\vspace{1mm}
\end{algorithm}

\end{frame}
%============================================================================
\begin{frame}{Value Iteration}

\textcolor{Maroon}{Drawback} of Policy Iteration: in each iteration we need to perform policy evaluation.

\vspace{2mm}

The policy evaluation step can be \textcolor{RoyalBlue}{truncated} without losing convergence guarantees.

\begin{align*}
    &V^{k+1}(s) = \max_a \;  \mathds{E} \bigg[\sum_{k}^{\infty}\gamma^{k} r_{t+k+1} \: \bigg| \: s_t=s, a_t = a \bigg] \\
		   & = \max_a \sum_{s_{t+1}} p(s_{t+1}|s,a)\big[r + \gamma V^{k}(s_{t+1}) \big] \; \text{for each } s \in \mathcal{S}
\end{align*}


\end{frame}


\begin{frame}{Value Iteration}


\begin{algorithm}[H]
\caption{Value Iteration Algorithm}

%\SetAlgoLined

\vspace{1mm}

Define threshold for accuracy $\theta$

Initialize $V(s)$ for all $s \in \mathcal{S}$ arbitrarily, except V(terminal) = 0

\vspace{4mm}

 $\Delta \leftarrow 0$

 \textbf{\text{while}} \text{True} \textbf{\text{do}}
 
 \vspace{0.5mm}
 
 \hspace{6mm} \textbf{\text{for}} \text{each} $s \in \mathcal{S}$ \textbf{\text{do}}
 
   
   \hspace{15mm} $v \leftarrow V(s)$
   
   \hspace{15mm} $V(s) \leftarrow \max_a \sum_{s_{t+1}}p(s_{t+1}|s,a)\Big[r+\gamma\:V(s_{t+1})\Big]$
   
   \hspace{15mm} $\Delta \leftarrow \max(\Delta, |v - V(s)|)$
   
   until $\Delta < \theta$

   \vspace{4mm}

   Output a deterministic policy $\pi \sim \pi^{*}$, such that:
   
   $\pi(s) \leftarrow \argmax_a \sum_{s_{t+1}}p(s_{t+1}|s,a)\Big[r+\gamma\:V(s_{t+1})\Big]$
   

\end{algorithm}

\end{frame}


%============================================================================
\begin{frame}{Generalised Policy Iteration}

We define generalized policy iteration as the \textcolor{RoyalBlue}{alternative execution of policy evaluation and policy improvement}, regardless of the granularity of the two processes.

\hspace{2mm}

This process stabilizes only when we found a policy that is greedy with respect to its own value function.

\end{frame}


\begin{frame}{Final Slide!}
	\begin{takeaway}{Lecture Takeaway}
		\begin{enumerate}
			\item Given $p(s_{t+1}|s,a)$ we can find the optimal policy mathematically
			\item But better use Dynamic Programming algorithms
			\item It works by alternating the tasks of Policy Evaluation and Control
		\end{enumerate}
	\end{takeaway}
\end{frame}


\end{document}
